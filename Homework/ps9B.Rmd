---
title: "STAT 231: Problem Set 9B"
author: "Thai Nguyen"
date: "due by 5 PM on Friday, November 13"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

This homework assignment is designed to help you further ingest, practice, and expand upon the material covered in class over the past week(s).  You are encouraged to work with other students, but all code and text must be written by you, and you must indicate below who you discussed the assignment with (if anyone).  

Steps to proceed:

\begin{enumerate}
\item In RStudio, go to File > Open Project, navigate to the folder with the course-content repo, select the course-content project (course-content.Rproj), and click "Open" 
\item Pull the course-content repo (e.g. using the blue-ish down arrow in the Git tab in upper right window)
\item Copy ps9B.Rmd from the course repo to your repo (see page 6 of the GitHub Classroom Guide for Stat231 if needed)
\item Close the course-content repo project in RStudio
\item Open YOUR repo project in RStudio
\item In the ps9B.Rmd file in YOUR repo, replace "YOUR NAME HERE" with your name
\item Add in your responses, committing and pushing to YOUR repo in appropriate places along the way
\item Run "Knit PDF" 
\item Upload the pdf to Gradescope.  Don't forget to select which of your pages are associated with each problem.  \textit{You will not get credit for work on unassigned pages (e.g., if you only selected the first page but your solution spans two pages, you would lose points for any part on the second page that the grader can't see).} 
\end{enumerate}

```{r, setup, include=FALSE}
library(tidyverse)
library(mdsr)
library(Lahman)
library(GGally)

knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
```


\newpage 
# If you discussed this assignment with any of your peers, please list who here:

> ANSWER:

\newpage
# 1. MDSR Exercise 9.5

Baseball players are voted into the Hall of Fame by the members of the Baseball Writers of America Association.  Quantitative criteria are used by the voters, but they are also allowed wide discretion.  The following code identifies the position players who have been elected to the Hall of Fame and tabulates a few basic statistics, include their number of career hits (`tH`), home runs (`tHR`), runs batted in (`tRBI`), and stolen bases (`tSB`).  Use the `kmeans()` function to perform a cluster analysis on these players.  Describe the properties that seem common to each cluster.

> ANSWER:  From the visualizations, it seems like cluster 2 performs the most poorly in terms of number of career hits and stolen bases. Meanwhile, cluster 1 performs the best out of them all in home runs and runs batted in, and also did pretty well in the other two categories. Cluster 3 did the worst in home runs and runs batted in, but have a generally diverse performance in terms of stolen bases and did the best in terms of number of career hits. 

```{r}
##### PLEASE DO NOT CHANGE THIS SEED NUMBER
##### keep set.seed(75) 
set.seed(75)

hof <- Batting %>%
  group_by(playerID) %>%
  inner_join(HallOfFame, by = "playerID") %>%
  filter(inducted == "Y" & votedBy == "BBWAA") %>%
  summarize(tH = sum(H), tHR = sum(HR), tRBI = sum(RBI), tSB = sum(SB)) %>%
  filter(tH > 1000)

# standardize the values
hof_std <- hof %>%
  mutate_if(is.numeric, funs(`std`=scale(.) %>% as.vector())) 

# To test how many clusters should be used in our k-means analysis
fig <- matrix(NA, nrow=10, ncol=2)

for (i in 1:10){
  fig[i,1] <- i
  fig[i,2] <- kmeans(hof_std[,6:9]
                    , centers=i
                    , nstart=20)$tot.withinss
}

ggplot(data = as.data.frame(fig), aes(x = V1, y = V2)) +
  geom_point() + 
  geom_line() +
  scale_x_continuous(breaks=c(1:10)) +
  labs(x = "K", y = expression("Total W"[k]))

ggplot(data = as.data.frame(fig[2:10,]), aes(x = V1, y = V2)) +
  geom_point() + 
  geom_line() +
  scale_x_continuous(breaks=c(1:10)) +
  labs(x = "K", y = expression("Total W"[k]))

# It seems like there is no clear elbow to the curve, but I would argue that going beyond 
# 3 clusters would not get us much reduction in distance from each player to the center.
# Thus, I will choose 3 clusters for the analysis.

vars <- c("tH_std", "tHR_std", "tRBI_std", "tSB_std")
km_bball <- kmeans(hof_std[,vars]
                        , centers=3
                        , nstart=20)

hof_clust <- hof_std %>%
  mutate(clust = as.character(km_bball$cluster))

ggpairs(data = hof_clust
        , aes(color = clust) 
        , columns = vars)
```


\newpage
# 2. MDSR Exercise 10.6 

\textit{Equal variance assumption}:  What is the impact of the violation of the equal variance assumption for linear regression models?  Repeatedly generate data from a "true" model given by the following code.  (Note that the standard deviation is dependent upon x2, which is random; i.e., the equal variance assumption is violated.  The Ys are *not* generated from a distribution with the same variance.)

For each simulation, fit the linear regression model and display the distribution of 1,000 estimates of the $\beta_1$ parameter.  Does the distribution of the estimates follow a normal distribution?

> ANSWER:  It seems like the distribution of the estimates of beta 1 parameter still follows a normal distribution since our histogram indicates that (even though the tails are a bit different from a normal distribution but this might be because of sample size). The normality can also be checked with the ggplot as most of the estimates from the sample lies on the line. 

```{r}
# number of observations in each sample
n_obs <- 250

# parameters held constant
rmse <- 1
beta0 <- -1
beta1 <- 0.5
beta2 <- 1.5

# how to generate data
x1 <- rep(c(0,1), each=n_obs/2)
x2 <- runif(n_obs, min=0, max=5)
y <- beta0 + beta1*x1 + beta2*x2 + rnorm(n=n_obs, mean=0, sd=rmse + x2)

# fit model
mod <- lm(y ~ x1 + x2)

# extract beta1 coefficient
dat <- summary(mod)$coeff["x1","Estimate"]

# now, write code to repeatedly generate data, fit the model, and extract the beta coefficient (1,000 times)

# set seed for reproducibility
set.seed(20201112)
# number of simulations
n_sim <- 1000
beta1_est <- rep(NA, n_sim)

for (i in 1:n_sim){
  x1 <- rep(c(0,1), each=n_obs/2)
  x2 <- runif(n_obs, min=0, max=5)
  y <- beta0 + beta1*x1 + beta2*x2 + rnorm(n=n_obs, mean=0, sd=rmse + x2)
  mod <- lm(y ~ x1 + x2)
  dat <- summary(mod)$coeff["x1","Estimate"]
  beta1_est[i] <- dat
}

# visualize sampling distribution for beta1 (e.g. with histogram or density plot)
ggplot(data = as.data.frame(beta1_est), aes(x=beta1_est)) +
  geom_histogram(color = "black", fill = "grey") +
  labs(x = "Estimation of beta 1", y = "Number of Samples")

# can also check normality with qqplot
# suppose beta1_est is your results vector containing the beta1 estimates
ggplot(data = as.data.frame(beta1_est), aes(sample = beta1_est)) +
  stat_qq() + 
  stat_qq_line()
```



\newpage
# 3. MDSR Exercise 10.7

\textit{Skewed residuals}: What is the impact if the residuals from a linear regression model are skewed (and not from a normal distribution)?  Repeatedly generate data from a "true" model given by the parameters below.

For each simulation, fit the linear regression model and display the distribution of 1,000 estimates of the $\beta_1$ parameter.

> ANSWER:  It seems that the estimation of the beta 1 parameter also does not change by much even if the residuals are skewed, as evident from our histogram, which indicates the sampling distribution that is normal distribution (with mean around 0.5).

```{r}
# number of observations in each sample
n_obs <- 250

# parameters held constant
rmse <- 1
beta0 <- -1
beta1 <- 0.5
beta2 <- 1.5

# how to generate data
x1 <- rep(c(0,1), each=n_obs/2)
x2 <- runif(n_obs, min=0, max=5)
y <- beta0 + beta1*x1 + beta2*x2 + rexp(n=n_obs, rate=1/2)

# what does an exponential dist'n with rate = 1/2 look like? 
# very skewed!
rexp(n=10000, rate=1/2) %>%
  as.data.frame() %>%
  ggplot(aes(x=`.`)) +
    geom_histogram(color="black", fill="grey")

# set seed for reproducibility
set.seed(20201112)
n_sim <- 1000
beta1_est_2 <- rep(NA, n_sim)

for (i in 1:n_sim){
  x1 <- rep(c(0,1), each=n_obs/2)
  x2 <- runif(n_obs, min=0, max=5)
  y <- beta0 + beta1*x1 + beta2*x2 + rexp(n=n_obs, rate=1/2)
  mod <- lm(y ~ x1 + x2)
  dat <- summary(mod)$coeff["x1","Estimate"]
  beta1_est_2[i] <- dat
}

# visualize sampling distribution for beta1 (e.g. with histogram or density plot)
ggplot(data = as.data.frame(beta1_est_2), aes(x=beta1_est_2)) +
  geom_histogram(color = "black", fill = "grey") +
  labs(x = "Estimation of beta 1", y = "Number of Samples")
```


