---
title: "STAT 231: Problem Set 5A"
author: "Thai Nguyen"
date: "due by 5 PM on Monday, September 28"
output: pdf_document
---

In order to most effectively digest the textbook chapter readings -- and the  new R commands each presents -- series A homework assignments are designed to encourage you to read the textbook chapters actively and in line with the textbook's Prop Tip of page 33:

"\textbf{Pro Tip}: If you want to learn how to use a particular command, we highly recommend running the example code on your own" 

A more thorough reading and light practice of the textbook chapter prior to class allows us to dive quicker and deeper into the topics and commands during class.  Furthermore, learning a programming lanugage is like learning any other language -- practice, practice, practice is the key to fluency.  By having two assignments each week, I hope to encourage practice throughout the week.  A little coding each day will take you a long way!

*Series A assignments are intended to be completed individually.*  While most of our work in this class will be collaborative, it is important each individual completes the active readings.  The problems should be straightforward based on the textbook readings, but if you have any questions, feel free to ask me!

Steps to proceed:

\begin{enumerate}
\item In RStudio, go to File > Open Project, navigate to the folder with the course-content repo, select the course-content project (course-content.Rproj), and click "Open" 
\item Pull the course-content repo (e.g. using the blue-ish down arrow in the Git tab in upper right window)
\item Copy ps5A.Rmd from the course repo to your repo (see page 6 of the GitHub Classroom Guide for Stat231 if needed)
\item Close the course-content repo project in RStudio
\item Open YOUR repo project in RStudio
\item In the ps5A.Rmd file in YOUR repo, replace "YOUR NAME HERE" with your name
\item Add in your responses, committing and pushing to YOUR repo in appropriate places along the way
\item Run "Knit PDF" 
\item Upload the pdf to Gradescope.  Don't forget to select which of your pages are associated with each problem.  \textit{You will not get credit for work on unassigned pages (e.g., if you only selected the first page but your solution spans two pages, you would lose points for any part on the second page that the grader can't see).} 
\end{enumerate}

```{r, setup, include=FALSE}
library(tidyverse)
library(mdsr)
library(tidytext)
library(aRxiv)
library(wordcloud)

knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
```

\newpage
# 1. Text as Data

### a.
In Section 19.1.1, the `grep` and `grepl` functions are introduced for detecting a pattern in a character vector (like finding a needle in a haystack).  The following three calls look similar, but return different results.  Explain what the 6 returned records indicate in each case:

- `head(grepl("  MACBETH", macbeth))`
- `head(grep("  MACBETH", macbeth, value = TRUE))`
- `head(grep("  MACBETH", macbeth))`

(Yes, the textbook explains the differences in these commands/calls to these commands, but it can be helpful if you run the lines yourself as well to be sure they work as you'd expect and to inspect the results.) 

> ANSWER: For grep without value = TRUE, it returns the location as indices where you can find the lines for Macbeth. For grep with value = TRUE, it returns the actual lines as strings of text instead of just the location. For grepl, it returns a logical vector as long as the original text (the haystack), but it still subsets the original vector in the same way that grep does. (Still found the pattern).

```{r}
# defining "macbeth" object
macbeth_url <- "http://www.gutenberg.org/cache/epub/1129/pg1129.txt"
Macbeth_raw <- RCurl::getURL(macbeth_url)
data(Macbeth_raw)
#Macbeth_raw

# strsplit returns a list: we only want the first element
macbeth <- stringr::str_split(Macbeth_raw, "\r\n")[[1]]
class(macbeth)
length(macbeth)

### finding literal strings
head(grep("  MACBETH", macbeth))
head(grep("  MACBETH", macbeth, value = TRUE))
head(grepl("  MACBETH", macbeth))
```


### b.
Section 19.1.1 also introduces regular expressions.  Why do the two lines below differ in their results?

- `head(grep("MACBETH\\.", macbeth, value = TRUE))`
- `head(grep("MACBETH.", macbeth, value = TRUE))`

(Yes, the textbook explains the differences, but it can be helpful if you run the lines yourself as well to be sure they work as you'd expect and to inspect the results.) 

> ANSWER: Without the double slashes, the search lines will return the lines that have the word 'Macbeth'. However, with the double slashes, they will only return the lines that has the word 'Macbeth' followed by a full stop.

```{r}
head(grep("MACBETH\\.", macbeth, value = TRUE))
head(grep("MACBETH.", macbeth, value = TRUE))
```

### c. 

The `stringr` package from the `tidyverse` collection of packages, has functions that work equivalently as `grep` and `grepl`.  In particular:

- `str_detect(string=, pattern=)` is equivalent to `grepl(pattern=, x=)`
- `str_which(string=, pattern=)` is equivalent to `grep(pattern=, x=)`
- `str_subset(string=, pattern=)` is equivalent to `grep(pattern=, x=, value=TRUE)`

Uncomment and run the code below to ensure the same results are returned for each different pair (at least for the first six records).  In words, explain what overall pattern is being searched for (i.e., what does the pattern "MAC[B-Z]" indicate?)?

> ANSWER: The overall pattern that is being searched for is a pattern that starts with MAC and having the next letter to be from B to Z.

```{r}
# (1) str_detect(string=, pattern =) is equivalent to `grepl(pattern=, x=)`
 head(grepl("MAC[B-Z]", macbeth))
 head(str_detect(macbeth, "MAC[B-Z]"))

# (2) `str_which(string=, pattern=)` is equivalent to `grep(pattern=, x=)`
 head(grep("MAC[B-Z]", macbeth))
 head(str_which(macbeth,"MAC[B-Z]"))

# (3) `str_subset(string=, pattern=)` is equivalent to `grep(pattern=, x=, value=TRUE)`
 head(grep("MAC[B-Z]", macbeth, value = TRUE))
 head(str_subset(macbeth, "MAC[B-Z]"))
```

### d.  OPTIONAL

In section 19.2.2, the `wordcloud` package is used to create a word cloud based on text in abstracts from Data Science articles in arXiv (which is "a fast-growing electronic repository of preprints of scientific papers from many disciplines").  I've provided some code below to get you started coding along with the example.  *This part (d) will not be graded, but is included to encourage you to test and explore some of the code in the extended example.*  What words are included in `tidytexts`'s `stop_words` dataset?  Do you think all of these words should be considered stop words (i.e. excluded from analysis) in all scenarios?  Are there any that might be useful in some contexts?  

```{r}
# note that the tidytext, aRxiv, and wordcloud packages were loaded in the 
# setup code chunk at the top of the program

# arxiv_search() is from the aRxiv package
DataSciencePapers <- arxiv_search(
  query = '"Data Science"', 
  limit = 20000, 
  batchsize = 100
)

glimpse(DataSciencePapers)

DataSciencePapers <- DataSciencePapers %>%
  mutate(
    submitted = lubridate::ymd_hms(submitted) 
    , updated = lubridate::ymd_hms(updated)
    , field = str_extract(primary_category, "^[a-z,-]+")
    , Field = ifelse(field == "cs", "Computer Science", "Other discipline"))

# stop words dataset provided by the tidytext package
stop_words <- tidytext::stop_words

# the unnest_tokens function is from the tidytext package
arxiv_words <- DataSciencePapers %>%
  unnest_tokens(output = word, input = abstract, token = "words") %>%
  anti_join(stop_words, by = "word") %>%
  select(word, id)

arxiv_word_freqs <- arxiv_words %>%
  count(id, word, sort = TRUE) %>%
  select(word, n, id)


# the wordcloud function is from the wordcloud package
# you may also need to install the "tm" package in order to use the function
set.seed(1962)
wordcloud(DataSciencePapers$abstract, 
  max.words = 40, 
  scale = c(8, 1), 
  colors = topo.colors(n = 30), 
  random.color = TRUE)
```

