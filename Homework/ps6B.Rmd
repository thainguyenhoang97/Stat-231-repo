---
title: "STAT 231: Problem Set 6B"
author: "THAI NGUYEN"
date: "due by 5 PM on Friday, October 9"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

This homework assignment is designed to help you futher ingest, practice, and expand upon the material covered in class over the past week(s).  You are encouraged to work with other students, but all code and text must be written by you, and you must indicate below who you discussed the assignment with (if anyone).  

Steps to proceed:

\begin{enumerate}
\item In RStudio, go to File > Open Project, navigate to the folder with the course-content repo, select the course-content project (course-content.Rproj), and click "Open" 
\item Pull the course-content repo (e.g. using the blue-ish down arrow in the Git tab in upper right window)
\item Copy ps6B.Rmd from the course repo to your repo (see page 6 of the GitHub Classroom Guide for Stat231 if needed)
\item Close the course-content repo project in RStudio
\item Open YOUR repo project in RStudio
\item In the ps6B.Rmd file in YOUR repo, replace "YOUR NAME HERE" with your name
\item Add in your responses, committing and pushing to YOUR repo in appropriate places along the way
\item Run "Knit PDF" 
\item Upload the pdf to Gradescope.  Don't forget to select which of your pages are associated with each problem.  \textit{You will not get credit for work on unassigned pages (e.g., if you only selected the first page but your solution spans two pages, you would lose points for any part on the second page that the grader can't see).} 
\end{enumerate}

```{r, setup, include=FALSE}
library(tidyverse)
library(tidytext)
library(wordcloud)
library(textdata)

knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
```


\newpage 
# If you discussed this assignment with any of your peers, please list who here:

> ANSWER:

\newpage
# Trump Tweets

David Robinson, Chief Data Scientist at DataCamp, wrote a blog post ["Text analysis of Trump's tweets confirms he writes only the (angrier) Android half"](http://varianceexplained.org/r/trump-tweets/).

He provides a dataset with over 1,500 tweets from the account realDonaldTrump between 12/14/2015 and 8/8/2016.  We'll use this dataset to explore the tweeting behavior of realDonaldTrump during this time period.

First, read in the file. Note that there is a `TwitteR` package which provides an interface to the Twitter web API.  We'll use this R dataset David created using that package so that you don't have to set up Twitter authentication.  

```{r}
load(url("http://varianceexplained.org/files/trump_tweets_df.rda"))
```

## A little wrangling to warm-up

1a.  There are a number of variables in the dataset we won't need.  

- First, confirm that all the observations in the dataset are from the screen-name `realDonaldTrump`.  

- Then, create a new dataset called `tweets` that only includes the following variables:

- `text`
- `created`
- `statusSource`

```{r}
#confirm that observations are all from screen-name 'realDonaldTrump'
summary(trump_tweets_df$screenName)
test <- trump_tweets_df %>%
  filter(screenName == "realDonaldTrump")
summary(test$screenName)

#we confirmed all observations are from the screen name 'realDonaldTrump'
#create new dataset
tweets <- trump_tweets_df %>%
  select(text, created, statusSource)
```

\newpage
1b. Using the `statusSource` variable, compute the number of tweets from each source.  How many different sources are there?  How often are each used?

> ANSWER: There are 5 different sources. 1 tweet came from Instagram, 120 came from Twitter Web Client, 1 came from Twitter from Ipad,762 came from Twitter for Android, and 628 came from Twitter for iPhone.

```{r}
tweets %>%
  group_by(statusSource) %>%
  summarize(n())
```

\newpage
1c. We're going to compare the language used between the Android and iPhone sources, so only want to keep tweets coming from those sources.  Explain what the `extract` function (from the `tidyverse` package) is doing below.  (Note that "regex" stands for "regular expression".)

> ANSWER: Given a regular expression (in this case, statusSource observations that start with "Twitter for"), extract() turns each group after "Twitter for" into categories in a new column.

```{r}
tweets2 <- tweets %>%
  extract(col = statusSource, into = "source"
          , regex = "Twitter for (.*)<"
          , remove = FALSE) %>%
  filter(source %in% c("Android", "iPhone"))
```


\newpage
## How does the language of the tweets differ by source?  

2a. Create a word cloud for the top 50 words used in tweets sent from the Android.  Create a second word cloud for the top 50 words used in tweets sent from the iPhone.  How do these word clouds compare?  (Are there some common words frequently used from both sources? Are the most common words different between the sources?)

Don't forget to remove stop words before creating the word cloud.  Also remove the terms "https" and "t.co".

> ANSWER:  It seems like the most common words are slightly different between the two sources. The most common ones from Android are "realdonaldtrump", "hillary" and "trump", with most common adjective being "crooked". The frequency of the most common words in this set is also closer to one another. For the iPhone map, the most common ones are "makeamericagreatagain" and "trump2016", both of which seems to relate more to campaign slogans. "crooked" is still there, just in a much lower frequency. 

```{r, fig.width=8, fig.height=8}
pal <- brewer.pal(10, "Paired")

#create word frequencies for Android
word_frequencies_and <- tweets2 %>%
  filter(source == "Android") %>%
  unnest_tokens(output = word, input = text) %>%
  anti_join(stop_words, by = "word") %>%
  filter(word != "https") %>%
  filter(word != "t.co") %>%
  count(word, sort = TRUE)

#create wordcloud for Android
wordcloud(word_frequencies_and$word, word_frequencies_and$n
          , max.words=50
          # plot the words in a random order
          , random.order=T
          # specify proportion of words with 90 degree rotation
          , rot.per=.15
          # specify the range of the size of the words
          , scale=c(8,0.3)
          # colors words from least to most frequent
          , colors = pal
          # font family
          , family="sans")

#create word frequencies for iPhone
word_frequencies_iph <- tweets2 %>%
  filter(source == "iPhone") %>%
  unnest_tokens(output = word, input = text) %>%
  anti_join(stop_words, by = "word") %>%
  filter(word != "https") %>%
  filter(word != "t.co") %>%
  count(word, sort = TRUE)

#create wordcloud for Android
wordcloud(word_frequencies_iph$word, word_frequencies_iph$n
          , max.words=50
          # plot the words in a random order
          , random.order=T
          # specify proportion of words with 90 degree rotation
          , rot.per=.15
          # specify the range of the size of the words
          , scale=c(8,0.3)
          # colors words from least to most frequent
          , colors = pal
          # font family
          , family="sans")
```

\newpage
2b. Create a visualization that compares the top 10 *bigrams* appearing in tweets by each source (that is, facet by source).  After creating a dataset with one row per bigram, you should remove any rows that contain a stop word within the bigram.  

How do the top used bigrams compare between the two sources?

> ANSWER:  

```{r}
#create dataset for bigrams
tweets_bigrams <- tweets2 %>%
  unnest_tokens(output = word, input = text
                , token = "ngrams", n = 2) %>%
  #remove stop words by separating and comparing
  separate(col = word , into = c("word1", "word2"), sep = " ", remove = FALSE) %>%
  anti_join(stop_words, by = c("word1" = "word")) %>%
  anti_join(stop_words, by = c("word2" = "word")) %>%
  filter(word1 != "https", word2 != "https") %>%
  filter(word != "t.co" , word2 != "t.co") 

#create visualization
tweets_bigrams %>%
  group_by(source) %>%
  count(word, sort = TRUE) %>% 
  slice(1:10) %>%
  ggplot(aes(x = reorder(word,n), y = n, color = word, fill=word)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  facet_wrap(~source, nrow = 1, scales = "free") +
  labs(y = "Number of instances"
       , title="The most common bigrams in Trump's tweets by source of Android of iPhone") +
  guides(color = "none", fill = "none")


    
  
```


\newpage
2c. Consider the sentiment.  Compute the proportion of words among the tweets within each source classified as "angry" and the proportion of words classified as "joy"  based on the NRC lexicon.  How does the proportion of "angry" and "joy" words compare between the two sources?  What about "positive" and "negative" words?  

> ANSWER: From iphone, the proportion of "angry" and "joy" word is 1.78% and 1.28% of the words respectively, while from android, that is 1.52% and 1.11% respectively. From iphone, the proportion of "negative" and "positive" word is 2.61% and 2.08% respectively, while from android, that is 2.93% and 1.90% respectively. 

```{r}
nrc_lexicon <- get_sentiments("nrc")
afinn_lexicon <- get_sentiments("afinn")

#Get the total number of words in iphone and android
word_frequencies_iph %>%
  summarize(total = sum(n))
#total = 4605

word_frequencies_and %>%
  summarize(total = sum(n))
#total = 6958

#Calculate proportion of "joy" and "anger"
#For iphone
sent_iph_nrc <- word_frequencies_iph %>%
  left_join(nrc_lexicon, "word") %>%
  filter(sentiment == "anger" | sentiment == "joy") 

sent_iph_nrc %>%
  group_by(sentiment) %>%
  summarize(prop = n()/4605)

#For android
sent_and_nrc <- word_frequencies_and %>%
  left_join(nrc_lexicon, "word") %>%
  filter(sentiment == "anger" | sentiment == "joy")

sent_and_nrc %>%
  group_by(sentiment) %>%
  summarize(prop = n()/6958)

#Calculate proportion of "positive" and "negative" 
#For iphone
sent_iph_afinn <- word_frequencies_iph %>%
  left_join(afinn_lexicon, "word") %>%
  filter(!is.na(value)) %>%
  mutate(sentiment = case_when(value > 0 ~ "positive",
                               value < 0 ~ "negative")) 

sent_iph_afinn %>%
  group_by(sentiment) %>%
  summarize(prop = n()/4605)

#For android
sent_and_afinn <- word_frequencies_and %>%
  left_join(afinn_lexicon, "word") %>%
  filter(!is.na(value)) %>%
  mutate(sentiment = case_when(value > 0 ~ "positive",
                               value < 0 ~ "negative")) 

sent_and_afinn %>%
  group_by(sentiment) %>%
  summarize(prop = n()/6958)

#For 2d
sent_iph_afinn %>% 
  mutate(total = n*value) %>%
  summarize(overall = sum(total))

sent_and_afinn %>% 
  mutate(total = n*value) %>%
  summarize(overall = sum(total))



```


\newpage
2d. Lastly, based on your responses above, do you think there is evidence to support Robinson's claim that Trump only writes the (angrier) Android half of the tweets from realDonaldTrump?  In 2-4 sentences, please explain.

> ANSWER: It might not be true that Trump only writes the Android half, based on text analysis of sentiments. As we can see from above, in both iPhone and Android sources, the words that connotates anger and negative sentiments constitute a larger proportion, which means that from both sources the words seem to convey more anger than joy, more negative than positive. 

However, the last part of the R code above used the AFINN lexicon to calculate the total sentiments in terms of positive and negative values, and it turns out that the overall sentiment values of words from iPhone source is positive (131) while from the Android source is negative (-116), which indicates that maybe there is a greater degree of negative sentiments/hatred conveyed in the Android source. 

